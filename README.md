# Implementation of: Self-supervised Audio-and-Language Pre-training with Extremely Low-Resource Parallel Data

Model architecture is in __models.py__ and __modules.py__

Overall pre-training flow is in __main.py__ and __m2p_runner.py__

Downstream tasks training is in __m2p_finetune.py__

Data preprocessing scripts are in __preprocess/__

Tokenizer for text inputs is in __tokenizer/__

Configs for pre-training and fine-tuning are in __config/__

*The code will be cleaned up and the README will be refined as soon as possible.*
