# Implementation of: Self-supervised Audio-and-Language Pre-training with Extremely Low-Resource Parallel Data

Model architecture in __models.py__

Pre-training flow in __main.py__

Downstream tasks training in __m2p_finetune.py__

Data preprocessing in __preprocess/__

Tokenizer for text in __tokenizer/__

*The code will be cleaned up and the README will be refined as soon as possible.*
